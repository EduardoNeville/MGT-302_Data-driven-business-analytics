{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " !!! If you don't fill these fields, your homework does not count !!!<by/>\n",
    " #### first name and last name : Eduardo Neville\n",
    " #### sciper number : 314667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run cells by hitting `Shift` + `Enter` or `ctrl` + `Enter`. <br/>\n",
    "We highly recommend you to read each line of code carefully and try to understand what it exactly does. <br/>\n",
    "Just alter the parts that is between green comments and specified for you. Please do not change other parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft margin SVM\n",
    "### about the Data:<br/>\n",
    "The purpose of this project is to classify tumors into malignant or benign. The following dataset is constructed based on images of tumors. Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.\n",
    "For more details about the features of this dataset you can visit this link:\n",
    "https://scikit-learn.org/stable/datasets/index.html#breast-cancer-dataset<br/>\n",
    "This dataset contains 30 features and 1 label that is called target. We should find a proper hyperplane that separates malignant and benign samples.\n",
    "The original dataset labels is 0 and 1 and in the following code boxes we change it to -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890     0.0  \n",
       "1          0.2750                  0.08902     0.0  \n",
       "2          0.3613                  0.08758     0.0  \n",
       "3          0.6638                  0.17300     0.0  \n",
       "4          0.2364                  0.07678     0.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(np.c_[cancer[\"data\"], cancer[\"target\"]], columns = np.append(cancer[\"feature_names\"],[\"target\"]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.8804920913884 %\n",
      "71.8804920913884 %\n",
      "20.738137082601053 %\n",
      "20.738137082601053 %\n",
      "7.381370826010544 %\n",
      "7.381370826010544 %\n"
     ]
    }
   ],
   "source": [
    "cancer.target = np.where(cancer.target==0, -1, cancer.target) \n",
    "X_train ,X_test ,X_val ,y_train ,y_test ,y_val = None ,None ,None ,None ,None ,None\n",
    "################################################################################\n",
    "# TODO: using train_test_split package, split your data into 3 numpy array     #\n",
    "# called X_train, X_test, and X_val and also split the corresponding labels as #\n",
    "# y_train, y_test, and y_val. After spliting, the ratio of your data should be # \n",
    "# approximately like this:                                                     #\n",
    "#  Train : 72%     test : 20%       validation : 8%                            #\n",
    "################################################################################\n",
    "\n",
    "# Splitting the data into train, testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,train_size=0.72, random_state=42)\n",
    "# Splitting the testing set into testing and validation\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test,train_size=0.74, random_state=42)\n",
    "\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "print((X_train.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((y_train.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((X_test.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((y_test.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((X_val.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((y_val.shape[0]/cancer.data.shape[0]) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### soft margin SVM optimization:<br/>\n",
    "We add 1 at the beginning of each Xs data (X_train, X_val , ...) and then the bias will be calculated implicitly.\n",
    "Then you should minimize the following SVM loss function (using gradient descent) with changing parameters of model.<br>\n",
    "In this notation: \n",
    "\\begin{equation}\n",
    "x_i , y_i\n",
    "\\end{equation}\n",
    "refers to feature vector of the sample and the label of our training data<br>\n",
    "and this is SVM loss function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "J(W) = \\frac{1}{N} \\sum_{i=1}^{N}{L^{(i)}} + \\frac{\\lambda}{2} ||W||^2\\\\\n",
    "\\large\n",
    "L^{(i)} ={max(0, 1 - y_i(w^{T}x_i)})\n",
    "\\;\\\\\n",
    "\\end{equation} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(409, 31)\n",
      "(42, 31)\n",
      "(118, 31)\n"
     ]
    }
   ],
   "source": [
    "# >>>>>WARNING: RUN THIS CELL ONLY ONCE!<<<<<\n",
    "\n",
    "# adding 1s to the end of feature vectors to be multiplied by bias term of weights\n",
    "X_val = np.insert(X_val, 0, 1, axis=1)\n",
    "X_train = np.insert(X_train, 0, 1, axis=1)\n",
    "X_test = np.insert(X_test, 0, 1, axis=1)\n",
    "print(X_train.shape)  \n",
    "print(X_val.shape)  \n",
    "print(X_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following functions in SVM class. In the part that you should compute loss function of this class, you are not allowed to use \"for\" loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T16:30:29.561420Z",
     "start_time": "2020-03-12T16:30:29.538696Z"
    }
   },
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, n_features: int, std: float):\n",
    "        \"\"\"\n",
    "        n_features: number of features in (or the dimension of) each instance\n",
    "        std: standard deviation used in the initialization of the weights of svm\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        ################################################################################\n",
    "        # TODO: Initialize the weights of svm using random normal distribution with    #\n",
    "        # standard deviation equals to std.                                            #\n",
    "        ################################################################################\n",
    "\n",
    "        self.w = np.random.normal(loc=0, scale=std, size=n_features)\n",
    "\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "\n",
    "    def loss(self, X: np.ndarray, y: np.ndarray, reg_coeff: float):\n",
    "        \"\"\"\n",
    "        X: training instances as a 2d-array with shape (num_train, n_features)\n",
    "        y: labels corresponsing to the given training instances as a 1d-array with shape (num_train,)\n",
    "        reg_coeff: L2-regularization coefficient\n",
    "        \"\"\"\n",
    "        loss = 0.0\n",
    "        \n",
    "        #################################################################################\n",
    "        # TODO: Compute the hinge loss specified in the notebook and save it in the loss#                                                   # loss variable.                                                               #\n",
    "        # NOTE: YOU ARE NOT ALLOWED TO USE FOR LOOPS!                                   #\n",
    "        # Don't forget L2-regularization term in your implementation!                   #\n",
    "        #################################################################################\n",
    "        \n",
    "        # hinge loss\n",
    "        margins = y * np.dot(X, self.w)\n",
    "        loss = np.sum(np.maximum(0, 1 - margins))\n",
    "        \n",
    "        # add L2-regularization term\n",
    "        loss += 0.5 * reg_coeff * np.sum(self.w ** 2)\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        return loss\n",
    "        \n",
    "    def update_weights(self,  X: np.ndarray, y: np.ndarray, learning_rate: float , reg_coeff: float):\n",
    "        \"\"\"\n",
    "        Updates the weights of the svm using the gradient of computed loss with respect to the weights. \n",
    "        learning_rate: learning rate that will be used in gradient descent to update the weights\n",
    "        \"\"\"\n",
    "        ################################################################################\n",
    "        # TODO: Compute the gradient of loss computed above w.r.t the svm weights.     #\n",
    "        # and then update self.w with the computed gradient.                           #\n",
    "        # (don't forget learning rate and reg_coeff in update rule)                    #\n",
    "        # Don't forget L2-regularization term in your implementation!                  #\n",
    "        ################################################################################\n",
    "        \n",
    "        # compute gradient\n",
    "        margins = y * np.dot(X, self.w)\n",
    "        misclassified = margins < 1\n",
    "        d_w = -np.dot(X[misclassified].T, y[misclassified])\n",
    "        d_w /= X.shape[0]\n",
    "        d_w += reg_coeff * self.w\n",
    "        \n",
    "        # update weights\n",
    "        self.w -= learning_rate * d_w\n",
    "\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: Numpy 2d-array of instances\n",
    "        \"\"\"\n",
    "        y_pred = None\n",
    "        ################################################################################\n",
    "        # TODO: predict the labels for the instances in X and save them in y_pred.     #                                      #\n",
    "        ################################################################################\n",
    "\n",
    "        y_pred = np.sign(np.dot(X, self.w))\n",
    "\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell contains your hyper parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = 0.0001\n",
    "num_iters = 15000\n",
    "reg_coeff = 20\n",
    "learning_rate=1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell using your SVM class, we want to train our model for cancer data:<br/>\n",
    "In every iteration you should see your training loss decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss 410.509242, val acc 66.67%\n",
      "iteration 100, loss 397.977769, val acc 33.33%\n",
      "iteration 200, loss 386.936695, val acc 33.33%\n",
      "iteration 300, loss 382.848448, val acc 33.33%\n",
      "iteration 400, loss 381.264520, val acc 33.33%\n",
      "iteration 500, loss 380.572980, val acc 33.33%\n",
      "iteration 600, loss 379.953496, val acc 33.33%\n",
      "iteration 700, loss 379.336508, val acc 33.33%\n",
      "iteration 800, loss 378.719550, val acc 33.33%\n",
      "iteration 900, loss 378.103481, val acc 33.33%\n",
      "iteration 1000, loss 377.487624, val acc 33.33%\n",
      "iteration 1100, loss 376.879271, val acc 33.33%\n",
      "iteration 1200, loss 376.281809, val acc 33.33%\n",
      "iteration 1300, loss 375.687697, val acc 33.33%\n",
      "iteration 1400, loss 375.093598, val acc 33.33%\n",
      "iteration 1500, loss 374.499510, val acc 33.33%\n",
      "iteration 1600, loss 373.914212, val acc 33.33%\n",
      "iteration 1700, loss 373.345798, val acc 33.33%\n",
      "iteration 1800, loss 372.777396, val acc 33.33%\n",
      "iteration 1900, loss 372.209005, val acc 33.33%\n",
      "iteration 2000, loss 371.657634, val acc 33.33%\n",
      "iteration 2100, loss 371.108513, val acc 33.33%\n",
      "iteration 2200, loss 370.559406, val acc 33.33%\n",
      "iteration 2300, loss 370.010301, val acc 33.33%\n",
      "iteration 2400, loss 369.461208, val acc 33.33%\n",
      "iteration 2500, loss 368.912126, val acc 33.33%\n",
      "iteration 2600, loss 368.386181, val acc 33.33%\n",
      "iteration 2700, loss 367.862206, val acc 33.33%\n",
      "iteration 2800, loss 367.338240, val acc 33.33%\n",
      "iteration 2900, loss 366.814286, val acc 33.33%\n",
      "iteration 3000, loss 366.290341, val acc 33.33%\n",
      "iteration 3100, loss 365.766408, val acc 33.33%\n",
      "iteration 3200, loss 365.242484, val acc 33.33%\n",
      "iteration 3300, loss 364.718572, val acc 33.33%\n",
      "iteration 3400, loss 364.204249, val acc 33.33%\n",
      "iteration 3500, loss 363.692353, val acc 33.33%\n",
      "iteration 3600, loss 363.180467, val acc 33.33%\n",
      "iteration 3700, loss 362.668591, val acc 33.33%\n",
      "iteration 3800, loss 362.156725, val acc 33.33%\n",
      "iteration 3900, loss 361.644870, val acc 33.33%\n",
      "iteration 4000, loss 361.133025, val acc 33.33%\n",
      "iteration 4100, loss 360.621190, val acc 33.33%\n",
      "iteration 4200, loss 360.109365, val acc 33.33%\n",
      "iteration 4300, loss 359.597551, val acc 33.33%\n",
      "iteration 4400, loss 359.085747, val acc 33.33%\n",
      "iteration 4500, loss 358.573953, val acc 33.33%\n",
      "iteration 4600, loss 358.062170, val acc 33.33%\n",
      "iteration 4700, loss 357.550397, val acc 33.33%\n",
      "iteration 4800, loss 357.038634, val acc 33.33%\n",
      "iteration 4900, loss 356.526881, val acc 33.33%\n",
      "iteration 5000, loss 356.015138, val acc 33.33%\n",
      "iteration 5100, loss 355.503406, val acc 33.33%\n",
      "iteration 5200, loss 354.991684, val acc 33.33%\n",
      "iteration 5300, loss 354.480036, val acc 33.33%\n",
      "iteration 5400, loss 353.969088, val acc 33.33%\n",
      "iteration 5500, loss 353.458145, val acc 33.33%\n",
      "iteration 5600, loss 352.947632, val acc 33.33%\n",
      "iteration 5700, loss 352.442908, val acc 33.33%\n",
      "iteration 5800, loss 351.938194, val acc 33.33%\n",
      "iteration 5900, loss 351.433490, val acc 33.33%\n",
      "iteration 6000, loss 350.928796, val acc 33.33%\n",
      "iteration 6100, loss 350.424112, val acc 35.71%\n",
      "iteration 6200, loss 349.919438, val acc 35.71%\n",
      "iteration 6300, loss 349.414774, val acc 35.71%\n",
      "iteration 6400, loss 348.910121, val acc 35.71%\n",
      "iteration 6500, loss 348.405477, val acc 35.71%\n",
      "iteration 6600, loss 347.900844, val acc 35.71%\n",
      "iteration 6700, loss 347.396221, val acc 35.71%\n",
      "iteration 6800, loss 346.891608, val acc 35.71%\n",
      "iteration 6900, loss 346.387005, val acc 35.71%\n",
      "iteration 7000, loss 345.882412, val acc 38.10%\n",
      "iteration 7100, loss 345.377830, val acc 38.10%\n",
      "iteration 7200, loss 344.873257, val acc 40.48%\n",
      "iteration 7300, loss 344.368695, val acc 42.86%\n",
      "iteration 7400, loss 343.864142, val acc 42.86%\n",
      "iteration 7500, loss 343.359600, val acc 45.24%\n",
      "iteration 7600, loss 342.855068, val acc 45.24%\n",
      "iteration 7700, loss 342.350546, val acc 47.62%\n",
      "iteration 7800, loss 341.846034, val acc 47.62%\n",
      "iteration 7900, loss 341.341532, val acc 50.00%\n",
      "iteration 8000, loss 340.837041, val acc 52.38%\n",
      "iteration 8100, loss 340.332559, val acc 54.76%\n",
      "iteration 8200, loss 339.828088, val acc 54.76%\n",
      "iteration 8300, loss 339.323626, val acc 54.76%\n",
      "iteration 8400, loss 338.819175, val acc 54.76%\n",
      "iteration 8500, loss 338.314734, val acc 59.52%\n",
      "iteration 8600, loss 337.810303, val acc 61.90%\n",
      "iteration 8700, loss 337.305882, val acc 61.90%\n",
      "iteration 8800, loss 336.801471, val acc 61.90%\n",
      "iteration 8900, loss 336.297071, val acc 66.67%\n",
      "iteration 9000, loss 335.792680, val acc 66.67%\n",
      "iteration 9100, loss 335.288300, val acc 66.67%\n",
      "iteration 9200, loss 334.783930, val acc 69.05%\n",
      "iteration 9300, loss 334.279569, val acc 69.05%\n",
      "iteration 9400, loss 333.775219, val acc 69.05%\n",
      "iteration 9500, loss 333.270879, val acc 69.05%\n",
      "iteration 9600, loss 332.766550, val acc 69.05%\n",
      "iteration 9700, loss 332.262230, val acc 69.05%\n",
      "iteration 9800, loss 331.757920, val acc 69.05%\n",
      "iteration 9900, loss 331.253621, val acc 69.05%\n",
      "iteration 10000, loss 330.749331, val acc 71.43%\n",
      "iteration 10100, loss 330.245052, val acc 76.19%\n",
      "iteration 10200, loss 329.740783, val acc 78.57%\n",
      "iteration 10300, loss 329.236524, val acc 85.71%\n",
      "iteration 10400, loss 328.732275, val acc 85.71%\n",
      "iteration 10500, loss 328.228036, val acc 85.71%\n",
      "iteration 10600, loss 327.723807, val acc 85.71%\n",
      "iteration 10700, loss 327.219589, val acc 85.71%\n",
      "iteration 10800, loss 326.715380, val acc 85.71%\n",
      "iteration 10900, loss 326.212391, val acc 83.33%\n",
      "iteration 11000, loss 325.709515, val acc 83.33%\n",
      "iteration 11100, loss 325.206648, val acc 85.71%\n",
      "iteration 11200, loss 324.703794, val acc 88.10%\n",
      "iteration 11300, loss 324.203760, val acc 88.10%\n",
      "iteration 11400, loss 323.705456, val acc 88.10%\n",
      "iteration 11500, loss 323.207163, val acc 88.10%\n",
      "iteration 11600, loss 322.708878, val acc 88.10%\n",
      "iteration 11700, loss 322.210586, val acc 90.48%\n",
      "iteration 11800, loss 321.712304, val acc 92.86%\n",
      "iteration 11900, loss 321.214044, val acc 92.86%\n",
      "iteration 12000, loss 320.715801, val acc 92.86%\n",
      "iteration 12100, loss 320.217567, val acc 92.86%\n",
      "iteration 12200, loss 319.719344, val acc 92.86%\n",
      "iteration 12300, loss 319.221131, val acc 92.86%\n",
      "iteration 12400, loss 318.722913, val acc 90.48%\n",
      "iteration 12500, loss 318.224701, val acc 92.86%\n",
      "iteration 12600, loss 317.726505, val acc 92.86%\n",
      "iteration 12700, loss 317.228331, val acc 92.86%\n",
      "iteration 12800, loss 316.730168, val acc 92.86%\n",
      "iteration 12900, loss 316.232014, val acc 92.86%\n",
      "iteration 13000, loss 315.733871, val acc 92.86%\n",
      "iteration 13100, loss 315.235729, val acc 92.86%\n",
      "iteration 13200, loss 314.737587, val acc 92.86%\n",
      "iteration 13300, loss 314.239455, val acc 92.86%\n",
      "iteration 13400, loss 313.741351, val acc 92.86%\n",
      "iteration 13500, loss 313.243258, val acc 92.86%\n",
      "iteration 13600, loss 312.745174, val acc 92.86%\n",
      "iteration 13700, loss 312.247101, val acc 92.86%\n",
      "iteration 13800, loss 311.749035, val acc 92.86%\n",
      "iteration 13900, loss 311.250963, val acc 92.86%\n",
      "iteration 14000, loss 310.752900, val acc 92.86%\n",
      "iteration 14100, loss 310.254861, val acc 92.86%\n",
      "iteration 14200, loss 309.756837, val acc 92.86%\n",
      "iteration 14300, loss 309.258823, val acc 95.24%\n",
      "iteration 14400, loss 308.760820, val acc 95.24%\n",
      "iteration 14500, loss 308.262826, val acc 95.24%\n",
      "iteration 14600, loss 307.764827, val acc 95.24%\n",
      "iteration 14700, loss 307.266835, val acc 95.24%\n",
      "iteration 14800, loss 306.768859, val acc 95.24%\n",
      "iteration 14900, loss 306.270905, val acc 95.24%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "model = SVM(n_features=X_train.shape[1], std= std )\n",
    "loss_history = []\n",
    "loss_val_history = []\n",
    "for it in range(num_iters):\n",
    "    loss = model.loss(X_train, y_train, reg_coeff)\n",
    "    loss_val = model.loss(X_val, y_val, reg_coeff)\n",
    "    if it % 100 == 0:\n",
    "        val_preds =  model.predict(X_val)\n",
    "        print('iteration %d, loss %f, val acc %.2f%%' % (it, loss,  accuracy_score(y_val,val_preds) * 100))\n",
    "    model.update_weights(X_train, y_train, learning_rate , reg_coeff)\n",
    "    loss_history.append(loss)\n",
    "    loss_val_history.append(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####################################################################################\n",
    "# TODO: using matplotlib.pyplot package plot the training loss and validation loss #\n",
    "# using loss_loss_history and loss_val_history                                     #\n",
    "####################################################################################\n",
    "\n",
    "plt.plot(loss_history, label='training loss')\n",
    "plt.plot(loss_val_history, label='validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "####################################################################################\n",
    "#                                 END OF YOUR CODE                                 #\n",
    "####################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "With changing your hyper parameters, find a configuration of hyper parameters that cause your loss to increase after each iteration and then report that configuration in the next cell. Explain why our loss increases?\n",
    "Write your answer in \"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The reason for the loss to increase is that the regularization term in the loss function dominates the hinge loss term. With a high regularization coefficient, the SVM tries to minimize the norm of the weights, which results in setting the weights to small values. Since the hinge loss term is not minimized, the SVM does not correctly classify the training examples, leading to an increase in the loss function. Also, the learning rate is high enough to cause significant weight updates in each iteration, leading to overshooting the optimal solution.\n",
    "\n",
    "In summary, the above hyperparameters cause the SVM to overfit the training data, leading to high values of regularization and low values of hinge loss, which results in misclassification of training examples and a continuous increase in the loss function.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "std = \"0.001\" <br>\n",
    "num_iters = \"1000\"<br>\n",
    "reg_coeff = \"100\"<br>\n",
    "learning_rate = \"0.01\"<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15+"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
