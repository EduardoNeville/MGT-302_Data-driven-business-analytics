{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " !!! If you don't fill these fields, your homework does not count !!!<by/>\n",
    " #### first name and last name : Ã…ke Janson\n",
    " #### sciper number : 314487"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run cells by hitting `Shift` + `Enter` or `ctrl` + `Enter`. <br/>\n",
    "We highly recommend you to read each line of code carefully and try to understand what it exactly does. <br/>\n",
    "Just alter the parts that is between green comments and specified for you. Please do not change other parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft margin SVM\n",
    "### about the Data:<br/>\n",
    "The purpose of this project is to classify tumors into malignant or benign. The following dataset is constructed based on images of tumors. Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.\n",
    "For more details about the features of this dataset you can visit this link:\n",
    "https://scikit-learn.org/stable/datasets/index.html#breast-cancer-dataset<br/>\n",
    "This dataset contains 30 features and 1 label that is called target. We should find a proper hyperplane that separates malignant and benign samples.\n",
    "The original dataset labels is 0 and 1 and in the following code boxes we change it to -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(np.c_[cancer[\"data\"], cancer[\"target\"]], columns = np.append(cancer[\"feature_names\"],[\"target\"]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer.target = np.where(cancer.target==0, -1, cancer.target) \n",
    "X_train ,X_test ,X_val ,y_train ,y_test ,y_val = None ,None ,None ,None ,None ,None\n",
    "################################################################################\n",
    "# TODO: using train_test_split package, split your data into 3 numpy array     #\n",
    "# called X_train, X_test, and X_val and also split the corresponding labels as #\n",
    "# y_train, y_test, and y_val. After spliting, the ratio of your data should be # \n",
    "# approximately like this:                                                     #\n",
    "#  Train : 72%     test : 20%       validation : 8%                            #\n",
    "################################################################################\n",
    "\n",
    "#write your code here\n",
    "\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "print((X_train.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((y_train.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((X_test.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((y_test.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((X_val.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((y_val.shape[0]/cancer.data.shape[0]) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### soft margin SVM optimization:<br/>\n",
    "We add 1 at the beginning of each Xs data (X_train, X_val , ...) and then the bias will be calculated implicitly.\n",
    "Then you should minimize the following SVM loss function (using gradient descent) with changing parameters of model.<br>\n",
    "In this notation: \n",
    "\\begin{equation}\n",
    "x_i , y_i\n",
    "\\end{equation}\n",
    "refers to feature vector of the sample and the label of our training data<br>\n",
    "and this is SVM loss function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "J(W) = \\frac{1}{N} \\sum_{i=1}^{N}{L^{(i)}} + \\frac{\\lambda}{2} ||W||^2\\\\\n",
    "\\large\n",
    "L^{(i)} ={max(0, 1 - y_i(w^{T}x_i)})\n",
    "\\;\\\\\n",
    "\\end{equation} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>WARNING: RUN THIS CELL ONLY ONCE!<<<<<\n",
    "\n",
    "# adding 1s to the end of feature vectors to be multiplied by bias term of weights\n",
    "X_val = np.insert(X_val, 0, 1, axis=1)\n",
    "X_train = np.insert(X_train, 0, 1, axis=1)\n",
    "X_test = np.insert(X_test, 0, 1, axis=1)\n",
    "print(X_train.shape)  \n",
    "print(X_val.shape)  \n",
    "print(X_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following functions in SVM class. In the part that you should compute loss function of this class, you are not allowed to use \"for\" loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T16:30:29.561420Z",
     "start_time": "2020-03-12T16:30:29.538696Z"
    }
   },
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, n_features: int, std: float):\n",
    "        \"\"\"\n",
    "        n_features: number of features in (or the dimension of) each instance\n",
    "        std: standard deviation used in the initialization of the weights of svm\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        ################################################################################\n",
    "        # TODO: Initialize the weights of svm using random normal distribution with    #\n",
    "        # standard deviation equals to std.                                            #\n",
    "        ################################################################################\n",
    "\n",
    "        #write your code here\n",
    "\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "\n",
    "    def loss(self, X: np.ndarray, y: np.ndarray, reg_coeff: float):\n",
    "        \"\"\"\n",
    "        X: training instances as a 2d-array with shape (num_train, n_features)\n",
    "        y: labels corresponsing to the given training instances as a 1d-array with shape (num_train,)\n",
    "        reg_coeff: L2-regularization coefficient\n",
    "        \"\"\"\n",
    "        loss = 0.0\n",
    "        \n",
    "        #################################################################################\n",
    "        # TODO: Compute the hinge loss specified in the notebook and save it in the loss#                                                   # loss variable.                                                               #\n",
    "        # NOTE: YOU ARE NOT ALLOWED TO USE FOR LOOPS!                                   #\n",
    "        # Don't forget L2-regularization term in your implementation!                   #\n",
    "        #################################################################################\n",
    "        \n",
    "        #write your code here\n",
    "\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        return loss\n",
    "        \n",
    "    def update_weights(self,  X: np.ndarray, y: np.ndarray, learning_rate: float , reg_coeff: float):\n",
    "        \"\"\"\n",
    "        Updates the weights of the svm using the gradient of computed loss with respect to the weights. \n",
    "        learning_rate: learning rate that will be used in gradient descent to update the weights\n",
    "        \"\"\"\n",
    "        ################################################################################\n",
    "        # TODO: Compute the gradient of loss computed above w.r.t the svm weights.     #\n",
    "        # and then update self.w with the computed gradient.                           #\n",
    "        # (don't forget learning rate and reg_coeff in update rule)                    #\n",
    "        # Don't forget L2-regularization term in your implementation!                  #\n",
    "        ################################################################################\n",
    "        \n",
    "        #write your code here\n",
    "\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: Numpy 2d-array of instances\n",
    "        \"\"\"\n",
    "        y_pred = None\n",
    "        ################################################################################\n",
    "        # TODO: predict the labels for the instances in X and save them in y_pred.     #                                      #\n",
    "        ################################################################################\n",
    "\n",
    "        #write your code here\n",
    "\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell contains your hyper parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = 0.0001\n",
    "num_iters = 15000\n",
    "reg_coeff = 20\n",
    "learning_rate=1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell using your SVM class, we want to train our model for cancer data:<br/>\n",
    "In every iteration you should see your training loss decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "model = SVM(n_features=X_train.shape[1], std= std )\n",
    "loss_history = []\n",
    "loss_val_history = []\n",
    "for it in range(num_iters):\n",
    "    loss = model.loss(X_train, y_train, reg_coeff)\n",
    "    loss_val = model.loss(X_val, y_val, reg_coeff)\n",
    "    if it % 100 == 0:\n",
    "        val_preds =  model.predict(X_val)\n",
    "        print('iteration %d, loss %f, val acc %.2f%%' % (it, loss,  accuracy_score(y_val,val_preds) * 100))\n",
    "    model.update_weights(X_train, y_train, learning_rate , reg_coeff)\n",
    "    loss_history.append(loss)\n",
    "    loss_val_history.append(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "# TODO: using matplotlib.pyplot package plot the training loss and validation loss #\n",
    "# using loss_loss_history and loss_val_history                                     #\n",
    "####################################################################################\n",
    "\n",
    "#write your code here\n",
    "\n",
    "####################################################################################\n",
    "#                                 END OF YOUR CODE                                 #\n",
    "####################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "With changing your hyper parameters, find a configuration of hyper parameters that cause your loss to increase after each iteration and then report that configuration in the next cell. Explain why our loss increases?\n",
    "Write your answer in \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "std = \"\" <br>\n",
    "num_iters = \"\"<br>\n",
    "reg_coeff = \"\"<br>\n",
    "learning_rate = \"\"<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
